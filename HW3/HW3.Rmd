---
title: "W3"
author: "Joaquin Rodriguez"
date: "9/11/2017"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 9. This question involves the use of multiple linear regression on the Auto data set.

## (a) Produce a scatterplot matrix which includes all of the variables in the data set.

```{r}
library(ISLR)
library(tidyverse)
library(ggfortify)

pairs(Auto)
```

## (b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.

```{r}
Auto %>% 
  select(., -name) %>% 
  cor()
```

## (c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:

```{r}
lm <- 
  Auto %>%
  select(., -name) %>%
  lm(., formula = mpg ~ .)

lm.summary <- 
  lm %>%  summary()

lm.summary %>% print()
```

### i. Is there a relationship between the predictors and the response?
The Adjusted R`(r lm.summary$adj.r.squared`) of the model seems to signal a strong linear relationship between the the response and the some of the predictors.

### ii. Which predictors appear to have a statistically significant relationship to the response?
The predictors that are significant at p < 0.01 are the following:  
* displacement  
* weight  
* year  
* origin

### iii. What does the coefficient for the year variable suggest?
The coefficient estimate for 'year' suggest that as the age of the car increases the consumption of fuel increases. In fact, the coefficeint suggest that an increase of one year makes the mpg response to increase by 0.75.

### Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?

```{r}
autoplot(lm, which = 1:6, ncol = 3, label.size = 3)
```

The residuals vs Fitted values plot provide a strong indication of non-linearity in the data. In fact, we can observe a u-shape of the residuals, suggesting that there might be a quadratic relationship in the data.  
The residuals plot suggest the presence of large ourliers for fitted values around 30 to 40lm. Furthermore, from the plot we observe evident heteroscedasticity issues as the variability of the residuals increases as the fitted values increase.  
The leverage plot identifies some observations that have a significant leverage, defined as observations with leverage higher than `r round((7+1) / length(Auto$mpg),3)` ( (p + 1) / n  ). However, the plot suggest that observation `r which(hat(model.matrix(lm)) > 0.15)` in particular has a considerably high leverage.  

### Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

```{r}

Auto %>% 
  select(-name) %>% 
  cor()

Auto %>% 
  select(-name) %>% 
  pairs()

```

From the correlation matrix we observe the the variables "cylinder", "displacement", "horsepower", and "weight" are strongly correlated. Therefore, from my knowledge about cars, I decided that the two most significant variables to keep in the model are horsepower and weight.  

```{r}

models <- tribble(
  ~func, ~models,
  "lm", list(formula = mpg ~ .),
  "lm", list(formula = mpg ~ cylinders + horsepower + displacement + weight + year + origin),
  "lm", list(formula = mpg ~ cylinders + horsepower + weight + year + origin),
  "lm", list(formula = mpg ~ weight + year + origin),
  "lm" ,list(formula = mpg ~ weight * year * origin),
  "lm" ,list(formula = mpg ~ weight + year * origin),
  "lm" ,list(formula = mpg ~ weight * year + origin)
  )

models <- 
  models %>% 
  mutate(result =invoke_map(func, models, data = select(Auto, -name)))

models <- 
  models %>% 
  mutate(summary =map(result, summary))

models <- 
  models %>%
  mutate(radj = map(.$summary, `[`, c("adj.r.squared")) %>% unlist())

```


```{r}
models$summary[[5]]
autoplot(models$result[[5]], which = 1:6, ncol = 3, label.size = 3)
```

The model with formula = "mpg ~ weight \* year \* origin" appear to have all terms statistically significant, both single variables and all interaction terms combinations.


### Try a few different transformations of the variables, such as log(X), X^1/2, X^2. Comment on your findings.
```{r}
Auto %>% 
  select(mpg, weight, year, origin) %>% 
  pairs()
```

Observing the pairs plot we can observe how the data between mpg and weight suggest that the relationship between the variables is quadratic with a right skew.

```{r}
Auto <- 
  Auto %>% 
  mutate(log.weight = log(weight),
         exp.weight = weight^2,
         root.weight = weight^(1/2),
         
         log.year = log(year),
         exp.year = year^2,
         root.year = year^(1/2))

Auto %>% 
  select(mpg, year, log.year, exp.year, root.year) %>% 
  pairs()


Auto %>% 
  select(mpg, weight, log.weight, exp.weight, root.weight) %>% 
  pairs()

```

```{r}
models <- tribble(
  ~func, ~models,
  "lm" ,list(formula = mpg ~ weight * year * origin),
  
  "lm" ,list(formula = mpg ~ log.weight * year * origin),
  "lm" ,list(formula = mpg ~ root.weight * year * origin),
  "lm" ,list(formula = mpg ~ exp.weight * year * origin),
  
  "lm" ,list(formula = mpg ~ weight * log.year * origin),
  "lm" ,list(formula = mpg ~ weight * root.year * origin),
  "lm" ,list(formula = mpg ~ weight * exp.year * origin),
  
  "lm" ,list(formula = mpg ~ exp.weight * exp.year * origin),
  "lm" ,list(formula = mpg ~ root.weight * root.year * origin),
  "lm" ,list(formula = mpg ~ exp.weight * exp.year * origin)
    )

models <- 
  models %>% 
  mutate(result =invoke_map(func, models, data = select(Auto, -name)))

models <- 
  models %>% 
  mutate(summary =map(result, summary))

models <- 
  models %>%
  mutate(radj = map(.$summary, `[`, c("adj.r.squared")) %>% unlist())


models$summary[[2]] %>% print()
autoplot(models$result[[2]], which = 1:6, ncol = 3, label.size = 3)
```

As we can see from the pairs plot the log transformation of the weight makes the relationship with mpg close to linear. In fact, among all the fitted models with different transformations the one with "mpg ~ log.weight \* year \* origin" results the one with higher adjusted Rsquared. However, from the analysis of the residuals we still observe a non linear pattern. Furthermore, the standardized residuals do not comply with the normality assumption.

# 10. This question should be answered using the Weekly data set, which is part of the ISLR package. This data is similar in nature to the Smarket data from this chapterâ€™s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

## (a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

```{r}
attach(Weekly)
summary(Weekly)


Weekly %>% 
  select(., -Direction) %>%
  gather(., `Lag1`,`Lag2`,`Lag3`,`Lag4`,`Lag5`,`Volume`,`Today`, key = "Var", value = "Val") %>% 
  group_by(Year, Var) %>%
  summarise(., Val= mean(Val)) %>% 
  ggplot(aes(x = Year, y = Val, color = Var)) +
  geom_line()


Weekly %>%
  select(., -Direction) %>% 
  cor()
```

All the lag variables on average assume  similar pattern over the weeks. Furthermore, Volume does not seem to have any evident effect on the Lag variables.

## Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
glm.fit <- 
  Weekly %>% 
  select(-Today, -Year) %>% 
  glm(Direction ~ ., family = binomial, data = .)

summary(glm.fit)

```

The only predictor that appears to be statitically significant is Lag2.

## Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

```{r}
glm.probs <- predict(glm.fit,type="response")

glm.pred <- rep("Down", 1089)
glm.pred[glm.probs >.5] <- "Up"

table(glm.pred,Direction)

# Fraction of correct predictions
(54+557) / 1089
```

From the confusion matrix and overall fraction of correct predictions we can observe how the accuracy of the logistic model is close to being that of a random guess. We can observe how the biggest misclassification is related to those stocks that were down.

## Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).
```{r}
glm.train <- 
  Weekly %>% 
  filter(Year > 1989 & Year > 2007) %>% 
  select(-Today, -Year) %>% 
  glm(Direction ~ ., family = binomial, data = .)

glm.probs <- predict(glm.fit,type="response", filter(Weekly,Year > 2008))

glm.pred <- rep("Down", 104)
glm.pred[glm.probs >.5] <- "Up"

table(glm.pred,filter(Weekly,Year > 2008)$Direction)

# Fraction of correct predictions
(17+48) /104
```

